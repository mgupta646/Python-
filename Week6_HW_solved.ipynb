{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e50351-15c7-4379-9369-cd41cd7ac272",
   "metadata": {},
   "source": [
    "# (Homework) Week 6 - DataScience Bootcamp Fall 2025\n",
    "\n",
    "All solution cells are replaced with `# TODO` placeholders so you can fill them in.\n",
    "\n",
    "**Name:** \\\n",
    "**Email:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ae2a1-9b4d-4b8e-87a8-fd32d8c107c8",
   "metadata": {},
   "source": [
    "### Problem 1: Dataset Splitting\n",
    "\n",
    "1. You have recordings of 44 phones from 100 people; each person records ~200 phones/day for 5 days.\n",
    "   - Design a valid training/validation/test split strategy that ensures the model generalizes to **new speakers**.\n",
    "\n",
    "2. You now receive an additional dataset of 10,000 phone recordings from **Kilian**, a single speaker.\n",
    "   - You must train a model that performs well **specifically for Kilian**, while also maintaining generalization.\n",
    "\n",
    "*Describe your proposed split strategy and reasoning.* (Theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cfdb6-aca2-4dd7-aaa4-70fa30af475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Problem 1 — Dataset Splitting (Speaker-Generalization + Kilian Adaptation)\n",
    "# This cell provides:\n",
    "# 1) A clear written plan (displayed as Markdown) for how to split the data to ensure generalization to new speakers.\n",
    "# 2) Reusable code utilities to perform strict speaker-level splits and a Kilian-specific split,\n",
    "#    with unit-test style checks that verify speaker exclusivity across splits.\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plan = r\"\"\"\n",
    "### Proposed Split Strategy (Speaker Independence + Kilian Adaptation)\n",
    "\n",
    "**Global (100 speakers) split — by speaker only (not by recording):**\n",
    "- Train: 70 speakers (all their days/recordings)\n",
    "- Validation: 15 speakers (all their days/recordings)\n",
    "- Test: 15 speakers (all their days/recordings)\n",
    "\n",
    "Rationale: Splitting *by speaker* ensures that validation/test contain **unseen people**, forcing generalization to new voices. \n",
    "Splitting within a speaker would leak vocal idiosyncrasies and inflate performance.\n",
    "\n",
    "**Kilian (single speaker, 10,000 recordings) — within-speaker split:**\n",
    "- Train: 80% (8,000)\n",
    "- Validation: 10% (1,000)\n",
    "- Test: 10% (1,000)\n",
    "\n",
    "**Training approach (two-stage):**\n",
    "1. Train a base model on the 100-speaker training set → learn robust, speaker-independent representations.\n",
    "2. Fine-tune (or adapt) on Kilian-train (8k) using a small learning rate / last layers / adapter blocks.\n",
    "   - Early stop on Kilian-val (1k).\n",
    "   - Evaluate on _both_ the global test (15 unseen speakers) and Kilian-test (1k).\n",
    "\n",
    "**Optional personalization:**\n",
    "- Add a speaker embedding (e.g., x-vector) or adapter tuned on Kilian.\n",
    "- Or multi-task: (global loss) + (Kilian-focused loss) with a small weight.\n",
    "\n",
    "**Reporting:**\n",
    "- Report **two metrics**: (a) Global test (unseen speakers) and (b) Kilian test.\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(plan))\n",
    "\n",
    "# --------------------------\n",
    "# Utilities for strict splits\n",
    "# --------------------------\n",
    "\n",
    "def speaker_level_split(\n",
    "    df: pd.DataFrame,\n",
    "    speaker_col: str = \"speaker_id\",\n",
    "    train_frac: float = 0.70,\n",
    "    val_frac: float = 0.15,\n",
    "    test_frac: float = 0.15,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits a metadata DataFrame into train/val/test by **speaker**.\n",
    "    Assumes df contains a column `speaker_col`. All rows for a given speaker go to exactly one split.\n",
    "    Returns: (df_train, df_val, df_test)\n",
    "    \"\"\"\n",
    "    assert abs(train_frac + val_frac + test_frac - 1.0) < 1e-8, \"Fractions must sum to 1.\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    speakers = df[speaker_col].dropna().unique().tolist()\n",
    "    rng.shuffle(speakers)\n",
    "\n",
    "    n = len(speakers)\n",
    "    n_train = int(round(train_frac * n))\n",
    "    n_val = int(round(val_frac * n))\n",
    "    # Ensure all speakers are assigned\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_speakers = set(speakers[:n_train])\n",
    "    val_speakers = set(speakers[n_train:n_train + n_val])\n",
    "    test_speakers = set(speakers[n_train + n_val:])\n",
    "\n",
    "    df_train = df[df[speaker_col].isin(train_speakers)].copy()\n",
    "    df_val   = df[df[speaker_col].isin(val_speakers)].copy()\n",
    "    df_test  = df[df[speaker_col].isin(test_speakers)].copy()\n",
    "\n",
    "    # Sanity checks: disjointness and coverage\n",
    "    assert train_speakers.isdisjoint(val_speakers)\n",
    "    assert train_speakers.isdisjoint(test_speakers)\n",
    "    assert val_speakers.isdisjoint(test_speakers)\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "def kilian_within_speaker_split(\n",
    "    df_kilian: pd.DataFrame,\n",
    "    train_frac: float = 0.80,\n",
    "    val_frac: float = 0.10,\n",
    "    test_frac: float = 0.10,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits Kilian's single-speaker data within-speaker by rows (recordings).\n",
    "    Returns: (kil_train, kil_val, kil_test)\n",
    "    \"\"\"\n",
    "    assert abs(train_frac + val_frac + test_frac - 1.0) < 1e-8, \"Fractions must sum to 1.\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    idx = np.arange(len(df_kilian))\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    n = len(idx)\n",
    "    n_train = int(round(train_frac * n))\n",
    "    n_val = int(round(val_frac * n))\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_idx = idx[:n_train]\n",
    "    val_idx   = idx[n_train:n_train + n_val]\n",
    "    test_idx  = idx[n_train + n_val:]\n",
    "\n",
    "    kil_train = df_kilian.iloc[train_idx].copy()\n",
    "    kil_val   = df_kilian.iloc[val_idx].copy()\n",
    "    kil_test  = df_kilian.iloc[test_idx].copy()\n",
    "\n",
    "    return kil_train, kil_val, kil_test\n",
    "\n",
    "\n",
    "# Example (toy) usage with synthetic metadata (run to sanity-check the split logic).\n",
    "if __name__ == \"__main__\":\n",
    "    # Create synthetic metadata for 100 speakers, 5 days, ~200 rec/day\n",
    "    rows = []\n",
    "    for spk in range(100):\n",
    "        for day in range(1, 6):\n",
    "            for r in range(200):\n",
    "                rows.append({\"speaker_id\": f\"S{spk:03d}\", \"day\": day, \"rec_id\": f\"S{spk:03d}_d{day}_r{r}\"})\n",
    "    meta = pd.DataFrame(rows)\n",
    "\n",
    "    df_tr, df_va, df_te = speaker_level_split(meta, \"speaker_id\", 0.70, 0.15, 0.15, random_state=123)\n",
    "\n",
    "    # Speaker exclusivity checks:\n",
    "    assert set(df_tr.speaker_id).isdisjoint(set(df_va.speaker_id))\n",
    "    assert set(df_tr.speaker_id).isdisjoint(set(df_te.speaker_id))\n",
    "    assert set(df_va.speaker_id).isdisjoint(set(df_te.speaker_id))\n",
    "\n",
    "    # Kilian synthetic 10k rows\n",
    "    kilian_meta = pd.DataFrame({\"speaker_id\": [\"Kilian\"]*10_000, \"day\": np.random.randint(1,6,10_000), \"rec_id\": [f\"K_{i}\" for i in range(10_000)]})\n",
    "    ktr, kva, kte = kilian_within_speaker_split(kilian_meta, 0.80, 0.10, 0.10, random_state=123)\n",
    "\n",
    "    print(\"Global split shapes:\", tuple(map(len, (df_tr, df_va, df_te))))\n",
    "    print(\"Kilian split shapes:\", tuple(map(len, (ktr, kva, kte))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b7930-1fef-4fd2-ac71-1467e8b165e8",
   "metadata": {},
   "source": [
    "### Problem 2: K-Nearest Neighbors\n",
    "\n",
    "1. **1-NN Classification:** Given dataset:\n",
    "\n",
    "   Positive: (1,2), (1,4), (5,4)\n",
    "\n",
    "   Negative: (3,1), (3,2)\n",
    "\n",
    "   Plot the 1-NN decision boundary and classify new points visually.\n",
    "\n",
    "2. **Feature Scaling:** Consider dataset:\n",
    "\n",
    "   Positive: (100,2), (100,4), (500,4)\n",
    "\n",
    "   Negative: (300,1), (300,2)\n",
    "\n",
    "   What would the 1-NN classify point (500,1) as **before and after scaling** to [0,1] per feature?\n",
    "\n",
    "3. **Handling Missing Values:** How can you modify K-NN to handle missing features in a test point?\n",
    "\n",
    "4. **High-dimensional Data:** Why can K-NN still work well for images even with thousands of pixels?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f66d2-4e36-4e30-8ef5-72d9b7986ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Problem 2 — K-Nearest Neighbors: 1-NN boundary + Feature Scaling demo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Dataset A (small coordinates)\n",
    "Xa_pos = np.array([[1,2],[1,4],[5,4]])\n",
    "Xa_neg = np.array([[3,1],[3,2]])\n",
    "Xa = np.vstack([Xa_pos, Xa_neg])\n",
    "ya = np.array([1]*len(Xa_pos) + [0]*len(Xa_neg))\n",
    "\n",
    "# 1-NN decision boundary for Dataset A\n",
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn1.fit(Xa, ya)\n",
    "\n",
    "# grid for plotting\n",
    "h = 0.05\n",
    "x_min, x_max = Xa[:,0].min()-1, Xa[:,0].max()+1\n",
    "y_min, y_max = Xa[:,1].min()-1, Xa[:,1].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = knn1.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, Z, alpha=0.2)\n",
    "plt.scatter(Xa_pos[:,0], Xa_pos[:,1], marker='o', label='Positive')\n",
    "plt.scatter(Xa_neg[:,0], Xa_neg[:,1], marker='x', label='Negative')\n",
    "plt.legend()\n",
    "plt.title(\"Problem 2 — 1-NN Decision Boundary (Dataset A)\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.show()\n",
    "\n",
    "# Dataset B (same geometry but x1 scaled ~100x)\n",
    "Xb_pos = np.array([[100,2],[100,4],[500,4]])\n",
    "Xb_neg = np.array([[300,1],[300,2]])\n",
    "Xb = np.vstack([Xb_pos, Xb_neg])\n",
    "yb = np.array([1]*len(Xb_pos) + [0]*len(Xb_neg))\n",
    "\n",
    "# Without scaling\n",
    "knn_no_scale = KNeighborsClassifier(n_neighbors=1).fit(Xb, yb)\n",
    "\n",
    "x_min, x_max = Xb[:,0].min()-50, Xb[:,0].max()+50\n",
    "y_min, y_max = Xb[:,1].min()-1,  Xb[:,1].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 2.0),\n",
    "                     np.arange(y_min, y_max, 0.05))\n",
    "Z = knn_no_scale.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, Z, alpha=0.2)\n",
    "plt.scatter(Xb_pos[:,0], Xb_pos[:,1], marker='o', label='Positive')\n",
    "plt.scatter(Xb_neg[:,0], Xb_neg[:,1], marker='x', label='Negative')\n",
    "plt.legend()\n",
    "plt.title(\"Problem 2 — 1-NN on Dataset B (No Scaling)\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.show()\n",
    "\n",
    "# With Standardization\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=1))])\n",
    "pipe.fit(Xb, yb)\n",
    "\n",
    "Zs = pipe.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.contourf(xx, yy, Zs, alpha=0.2)\n",
    "plt.scatter(Xb_pos[:,0], Xb_pos[:,1], marker='o', label='Positive')\n",
    "plt.scatter(Xb_neg[:,0], Xb_neg[:,1], marker='x', label='Negative')\n",
    "plt.legend()\n",
    "plt.title(\"Problem 2 — 1-NN on Dataset B (With StandardScaler)\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Without scaling, the large x1 range dominates distance; with scaling, axes contribute comparably.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f766e-e313-4c28-a2af-b8a7985e3db7",
   "metadata": {},
   "source": [
    "### Problem 3: Part 1\n",
    "\n",
    "You are given a fully trained Perceptron model with weight vector **w**, along with training set **D_TR** and test set **D_TE**.\n",
    "\n",
    "1. Your co-worker suggests evaluating $h(x) = sign(w \\cdot x)$ for every $(x, y)$ in D_TR and D_TE. Does this help determine whether test error is higher than training error?\n",
    "2. Why is there no need to compute training error explicitly for the Perceptron algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca95dc-c37e-4f56-ab0a-9913bde3079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Problem 3 — Part 1 (Perceptron evaluation theory)\n",
    "from IPython.display import Markdown, display\n",
    "txt = r\"\"\"\n",
    "**1) Is evaluating only on $D_{TE}$ enough?**  \n",
    "No. You should report **both** training and test performance. For a Perceptron trained on linearly separable data, the\n",
    "training error becomes **zero** upon convergence. However, zero training error does **not** imply low test error. \n",
    "Generalization depends on properties like margin, data radius, and sample size—not simply the achieved training error.\n",
    "Use a held-out validation/test set to estimate generalization.\n",
    "\n",
    "**Margin bound intuition:** If the data are separable with margin $\\gamma$ and inputs are bounded by radius $R$,\n",
    "the Perceptron makes at most $(R/\\gamma)^2$ mistakes during training. This is **not** a direct test-error bound, but it\n",
    "explains why larger margins typically correlate with better generalization.\n",
    "\n",
    "**2) Why don't we need to compute training error explicitly?**  \n",
    "After the Perceptron **converges** on separable data, it classifies all training examples correctly; hence the training\n",
    "error is identically **0%**. If the data are **not** separable, the algorithm does not converge; in that case, report the\n",
    "empirical training error by evaluating $\\mathrm{sign}(w \\cdot x)$ on $D_{TR}$ (but typical Perceptron training loops\n",
    "monitor mistakes per epoch anyway).\n",
    "\"\"\"\n",
    "display(Markdown(txt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e8682-2b9f-4b15-a38e-2d3ec75591dc",
   "metadata": {},
   "source": [
    "### Problem 3: Two-point 2D Dataset (Part 2)\n",
    "\n",
    "Run the Perceptron algorithm **by hand or in code** on the following data:\n",
    "\n",
    "1. Positive class: (10, -2)\n",
    "2. Negative class: (12, 2)\n",
    "\n",
    "Start with $w_0 = (0, 0)$ and a learning rate of 1.\n",
    "\n",
    "- Compute how many updates are required until convergence.\n",
    "- Write down the sequence of $w_i$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4597a-387e-4d5d-bbe3-f621afd13625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Problem 3 — Part 2: Perceptron on two points\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[10, -2],   # positive\n",
    "              [12,  2]])  # negative\n",
    "y = np.array([+1, -1])\n",
    "\n",
    "w = np.array([0.0, 0.0])\n",
    "lr = 1.0\n",
    "\n",
    "history = [w.copy()]\n",
    "updates = 0\n",
    "\n",
    "def predict(w, x):\n",
    "    s = np.dot(w, x)\n",
    "    return 1 if s >= 0 else -1\n",
    "\n",
    "converged = False\n",
    "# Iterate with a small safety cap\n",
    "for epoch in range(100):\n",
    "    mistakes = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        if predict(w, xi) != yi:\n",
    "            w = w + lr * yi * xi\n",
    "            history.append(w.copy())\n",
    "            updates += 1\n",
    "            mistakes += 1\n",
    "    if mistakes == 0:\n",
    "        converged = True\n",
    "        break\n",
    "\n",
    "print(\"Converged:\", converged)\n",
    "print(\"Total updates:\", updates)\n",
    "print(\"Sequence of w:\")\n",
    "for i, wi in enumerate(history):\n",
    "    print(f\"w_{i} = {wi}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba29c20-59b0-456f-994e-05897175596e",
   "metadata": {},
   "source": [
    "### Problem 4: Reconstructing the Weight Vector\n",
    "\n",
    "Given the log of Perceptron updates:\n",
    "\n",
    "| x | y | count |\n",
    "|---|---|--------|\n",
    "| (0, 0, 0, 0, 4) | +1 | 2 |\n",
    "| (0, 0, 6, 5, 0) | +1 | 1 |\n",
    "| (3, 0, 0, 0, 0) | -1 | 1 |\n",
    "| (0, 9, 3, 6, 0) | -1 | 1 |\n",
    "| (0, 1, 0, 2, 5) | -1 | 1 |\n",
    "\n",
    "Assume learning rate = 1 and initial weight $w_0 = (0, 0, 0, 0, 0)$.\n",
    "\n",
    "Compute the final weight vector after all updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb261e-d6ba-4ecd-a4f4-e9b6f5104079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Problem 4 — Reconstruct final Perceptron weight vector from update log\n",
    "import numpy as np\n",
    "\n",
    "# Each entry: (x, y, count)\n",
    "entries = [\n",
    "    ((0, 0, 0, 0, 4), +1, 2),\n",
    "    ((0, 0, 6, 5, 0), +1, 1),\n",
    "    ((3, 0, 0, 0, 0), -1, 1),\n",
    "    ((0, 9, 3, 6, 0), -1, 1),\n",
    "    ((0, 1, 0, 2, 5), -1, 1),\n",
    "]\n",
    "w = np.zeros(5, dtype=float)\n",
    "for x, y, c in entries:\n",
    "    w += c * y * np.array(x, dtype=float)\n",
    "\n",
    "print(\"Final weight vector w =\", w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f23b69-9f59-46c6-8103-5783fadeb7c0",
   "metadata": {},
   "source": [
    "### Problem 5: Visualizing Perceptron Convergence\n",
    "\n",
    "Implement a Perceptron on a small 2D dataset with positive and negative examples.\n",
    "\n",
    "- Plot the data points.\n",
    "- After each update, visualize the decision boundary.\n",
    "- Show how it converges to a stable separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879a3a9-de75-40a0-a901-bd2009d2b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Problem 5 — Visualizing Perceptron Convergence on a 2D toy set\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple linearly separable set\n",
    "pos = np.array([[2,2],[3,3],[2.5,3.5]])\n",
    "neg = np.array([[0,1],[1,0],[1,1]])\n",
    "\n",
    "X = np.vstack([pos, neg])\n",
    "y = np.array([+1]*len(pos) + [-1]*len(neg))\n",
    "\n",
    "w = np.zeros(2, dtype=float)\n",
    "b = 0.0\n",
    "lr = 1.0\n",
    "\n",
    "def predict_raw(w, b, X):\n",
    "    return X @ w + b\n",
    "\n",
    "def sign(z):\n",
    "    return np.where(z >= 0, 1, -1)\n",
    "\n",
    "def plot_boundary(w, b, title):\n",
    "    # grid\n",
    "    x_min, x_max = X[:,0].min()-0.5, X[:,0].max()+0.5\n",
    "    y_min, y_max = X[:,1].min()-0.5, X[:,1].max()+0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    Z = predict_raw(w, b, np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, (Z>=0).astype(int), alpha=0.2)\n",
    "    plt.scatter(pos[:,0], pos[:,1], marker='o', label='Positive')\n",
    "    plt.scatter(neg[:,0], neg[:,1], marker='x', label='Negative')\n",
    "    plt.contour(xx, yy, Z, levels=[0], linewidths=2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Initial boundary\n",
    "plot_boundary(w, b, \"Perceptron — Initial boundary\")\n",
    "\n",
    "# Train; after each mistake, replot (limit to a few for brevity)\n",
    "plots = 0\n",
    "max_plots = 4\n",
    "for epoch in range(10):\n",
    "    mistakes = 0\n",
    "    for xi, yi in zip(X, y):\n",
    "        if yi * (np.dot(w, xi) + b) <= 0:\n",
    "            w = w + lr * yi * xi\n",
    "            b = b + lr * yi\n",
    "            mistakes += 1\n",
    "            if plots < max_plots:\n",
    "                plot_boundary(w, b, f\"After update {plots+1}: w={w}, b={b}\")\n",
    "                plots += 1\n",
    "    if mistakes == 0:\n",
    "        break\n",
    "\n",
    "# Final boundary\n",
    "plot_boundary(w, b, \"Perceptron — Final boundary\")\n",
    "print(\"Final parameters:\", w, b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
